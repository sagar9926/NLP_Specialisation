{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla RNNs, GRUs and the scan function.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNi4nSaysiGfpSqQWutvsHN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagar9926/NLP_Specialisation/blob/main/SequenceModelling/Vanilla_RNNs%2C_GRUs_and_the_scan_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vPCIiLDFc6q"
      },
      "source": [
        "# Vanilla RNNs, GRUs and the `scan` function\n",
        "\n",
        "In this notebook, you will learn how to define the forward method for vanilla RNNs and GRUs. Additionally, you will see how to define and use the function `scan` to compute forward propagation for RNNs.\n",
        "\n",
        "By completing this notebook, you will:\n",
        "\n",
        "- Be able to define the forward method for vanilla RNNs and GRUs\n",
        "- Be able to define the `scan` function to perform forward propagation for RNNs\n",
        "- Understand how forward propagation is implemented for RNNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBBaUd-TCO4a"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "from time import perf_counter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhXTVOzLFzzY"
      },
      "source": [
        "def sigmoid(x): # Sigmoid function\n",
        "    return 1.0 / (1.0 + np.exp(-x))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJjG7kVEHKnx"
      },
      "source": [
        "# Part 1: Forward method for vanilla RNNs and GRUs\n",
        "\n",
        "In this part of the notebook, you'll see the implementation of the forward method for a vanilla RNN and you'll implement that same method for a GRU. For this excersice you'll use a set of random weights and variables with the following dimensions:\n",
        "\n",
        "- Embedding size (`emb`) : 128\n",
        "- Hidden state size (`h_dim`) : (16,1)\n",
        "\n",
        "The weights `w_` and biases `b_` are initialized with dimensions (`h_dim`, `emb + h_dim`) and (`h_dim`, 1). We expect the hidden state `h_t` to be a column vector with size (`h_dim`,1) and the initial hidden state `h_0` is a vector of zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA_OwN2-F1TC"
      },
      "source": [
        "random.seed(10)                 # Random seed, so your results match ours\n",
        "emb = 128                       # Embedding size\n",
        "T = 256                         # Number of variables in the sequences\n",
        "h_dim = 16                      # Hidden state dimension\n",
        "h_0 = np.zeros((h_dim, 1))      # Initial hidden state\n",
        "# Random initialization of weights and biases\n",
        "w1 = random.standard_normal((h_dim, emb+h_dim))\n",
        "w2 = random.standard_normal((h_dim, emb+h_dim))\n",
        "w3 = random.standard_normal((h_dim, emb+h_dim))\n",
        "b1 = random.standard_normal((h_dim, 1))\n",
        "b2 = random.standard_normal((h_dim, 1))\n",
        "b3 = random.standard_normal((h_dim, 1))\n",
        "X = random.standard_normal((T, emb, 1))\n",
        "weights = [w1, w2, w3, b1, b2, b3]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIgYMUYOHvw6"
      },
      "source": [
        "The vanilla RNN cell is quite straight forward. Its most general structure is presented in the next figure: \n",
        "\n",
        "<img src=\"https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/raw/d562105e68a0b85012ad3ebbb29b2af6344ad4e5/Natural%20Language%20Processing%20with%20Sequence%20Models/Week%202/RNN.PNG\" width=\"400\"/>\n",
        "\n",
        "As you saw in the lecture videos, the computations made in a vanilla RNN cell are equivalent to the following equations:\n",
        "\n",
        "\n",
        "$h^{<t>} = g(W_{h}[h^{<t-1>},x^{<t>}] + b_h) $\n",
        "\n",
        "${y^h}^{<t>}=g(W_{yh}h^{<t>} + b_y)$\n",
        "\n",
        "\n",
        "where $[h^{<t-1>},x^{<t>}]$ means that $h^{<t-1>}$ and $x^{<t>}$ are concatenated together. In the next cell we provide the implementation of the forward method for a vanilla RNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N-yeHqTHm--"
      },
      "source": [
        "def forward_V_RNN(inputs, weights): # Forward propagation for a a single vanilla RNN cell\n",
        "    x, h_t = inputs\n",
        "\n",
        "    # weights.\n",
        "    wh, _, _, bh, _, _ = weights\n",
        "\n",
        "    # new hidden state\n",
        "    h_t = np.dot(wh, np.concatenate([h_t, x])) + bh\n",
        "    h_t = sigmoid(h_t)\n",
        "\n",
        "    return h_t, h_t"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oovPXmVJLDF-"
      },
      "source": [
        "As you can see, we omitted the computation of $\\hat{y}^{<t>}$. This was done for the sake of simplicity, so you can focus on the way that hidden states are updated here and in the GRU cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omcu-5MbLGag"
      },
      "source": [
        "## 1.2 Forward method for GRUs\n",
        "\n",
        "A GRU cell have more computations than the ones that vanilla RNNs have. You can see this visually in the following diagram:\n",
        "\n",
        "<img src=\"https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/raw/d562105e68a0b85012ad3ebbb29b2af6344ad4e5/Natural%20Language%20Processing%20with%20Sequence%20Models/Week%202/GRU.PNG\" width=\"400\"/>\n",
        "\n",
        "As you saw in the lecture videos, GRUs have relevance $\\Gamma_r$ and update $\\Gamma_u$ gates that control how the hidden state $h^{<t>}$ is updated on every time step. With these gates, GRUs are capable of keeping relevant information in the hidden state even for long sequences. The equations needed for the forward method in GRUs are provided below: \n",
        "\n",
        "\n",
        "$\\Gamma_r=\\sigma{(W_r[h^{<t-1>}, x^{<t>}]+b_r)}$\n",
        "\n",
        "\n",
        "$\\Gamma_u=\\sigma{(W_u[h^{<t-1>}, x^{<t>}]+b_u)}$\n",
        "\n",
        "$c^{<t>}=\\tanh{(W_h[\\Gamma_r*h^{<t-1>},x^{<t>}]+b_h)}$\n",
        "\n",
        "$h^{<t>}=\\Gamma_u*c^{<t>}+(1-\\Gamma_u)*h^{<t-1>}$\n",
        "\n",
        "In the next cell, please implement the forward method for a GRU cell by computing the update `u` and relevance `r` gates, and the candidate hidden state `c`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk2Ddp2EHobP"
      },
      "source": [
        "def forward_GRU(inputs, weights): # Forward propagation for a single GRU cell\n",
        "    x, h_t = inputs\n",
        "\n",
        "    # weights.\n",
        "    wu, wr, wc, bu, br, bc = weights\n",
        "\n",
        "    # Update gate\n",
        "    ### START CODE HERE (1-2 lINES) ###\n",
        "    u = np.dot(wu, np.concatenate([h_t, x])) + bu\n",
        "    u = sigmoid(u)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Relevance gate\n",
        "    ### START CODE HERE (1-2 lINES) ###\n",
        "    r = np.dot(wr, np.concatenate([h_t, x])) + br\n",
        "    r = sigmoid(r)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Candidate hidden state \n",
        "    ### START CODE HERE (1-2 lINES) ###\n",
        "    c = np.dot(wc, np.concatenate([r * h_t, x])) + bc\n",
        "    c = np.tanh(c)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # New Hidden state h_t\n",
        "    h_t = u* c + (1 - u)* h_t\n",
        "    return h_t, h_t"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU53nPQQOWiv",
        "outputId": "5a23aedb-4cae-4c36-9d89-a205a34985a3"
      },
      "source": [
        "forward_GRU([X[0],h_0], weights)[0]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.51274490e-01],\n",
              "       [-9.99997455e-01],\n",
              "       [-1.24643746e-06],\n",
              "       [-9.99970502e-01],\n",
              "       [-9.35853020e-05],\n",
              "       [ 9.99746405e-01],\n",
              "       [ 4.18433337e-03],\n",
              "       [-9.99999345e-01],\n",
              "       [ 9.60126258e-04],\n",
              "       [ 6.49678310e-01],\n",
              "       [-9.56896518e-01],\n",
              "       [-3.21332511e-01],\n",
              "       [ 2.38414515e-02],\n",
              "       [-9.79230498e-01],\n",
              "       [ 6.74189998e-02],\n",
              "       [-1.74317466e-03]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9KzyKxzOepA"
      },
      "source": [
        "# Part 2: Implementation of the `scan` function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kaoh9xh4OhOX"
      },
      "source": [
        "In the lectures you saw how the `scan` function is used for forward propagation in RNNs. It takes as inputs:\n",
        "\n",
        "- `fn` : the function to be called recurrently (i.e. `forward_GRU`)\n",
        "- `elems` : the list of inputs for each time step (`X`)\n",
        "- `weights` : the parameters needed to compute `fn`\n",
        "- `h_0` : the initial hidden state\n",
        "\n",
        "`scan` goes through all the elements `x` in `elems`, calls the function `fn` with arguments ([`x`, `h_t`],`weights`), stores the computed hidden state `h_t` and appends the result to a list `ys`. Complete the following cell by calling `fn` with arguments ([`x`, `h_t`],`weights`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH7zFMW3OZeh"
      },
      "source": [
        "def scan(fn, elems, weights, h_0=None): # Forward propagation for RNNs\n",
        "    h_t = h_0\n",
        "    ys = []\n",
        "    for x in elems:\n",
        "        ### START CODE HERE (1 lINE) ###\n",
        "        y, h_t = fn([x, h_t], weights)\n",
        "        ### END CODE HERE ###\n",
        "        ys.append(y)\n",
        "    return ys, h_t"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL_qe63SPJR2"
      },
      "source": [
        "# Part 3: Comparison between vanilla RNNs and GRUs\n",
        "\n",
        "You have already seen how forward propagation is computed for vanilla RNNs and GRUs. As a quick recap, you need to have a forward method for the recurrent cell and a function like `scan` to go through all the elements from a sequence using a forward method. You saw that GRUs performed more computations than vanilla RNNs, and you can check that they have 3 times more parameters. In the next two cells, we compute forward propagation for a sequence with 256 time steps (`T`) for an RNN and a GRU with the same hidden state `h_t` size (`h_dim`=16).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqTgmoWyO9J_",
        "outputId": "bd235b92-287c-4f71-ae3f-b4609124ac5f"
      },
      "source": [
        "# vanilla RNNs\n",
        "tic = perf_counter()\n",
        "ys, h_T = scan(forward_V_RNN, X, weights, h_0)\n",
        "toc = perf_counter()\n",
        "RNN_time=(toc-tic)*1000\n",
        "print (f\"It took {RNN_time:.2f}ms to run the forward method for the vanilla RNN.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It took 5.16ms to run the forward method for the vanilla RNN.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHHbOJ5LPXdZ",
        "outputId": "2221f774-9925-44a7-89d0-dd2bbce79e04"
      },
      "source": [
        "# GRUs\n",
        "tic = perf_counter()\n",
        "ys, h_T = scan(forward_GRU, X, weights, h_0)\n",
        "toc = perf_counter()\n",
        "GRU_time=(toc-tic)*1000\n",
        "print (f\"It took {GRU_time:.2f}ms to run the forward method for the GRU.\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It took 16.53ms to run the forward method for the GRU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4or68MmHPmoY"
      },
      "source": [
        "As you were told in the lectures, GRUs take more time to compute (However, sometimes, although a rare occurrence, Vanilla RNNs take more time. Can you figure out what might cause this ?). This means that training and prediction would take more time for a GRU than for a vanilla RNN. However, GRUs allow you to propagate relevant information even for long sequences, so when selecting an architecture for NLP you should assess the tradeoff between computational time and performance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRphGKZaPsPA"
      },
      "source": [
        "# Creating a GRU model using Trax: Ungraded Lecture Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXrN0a2QPf8l",
        "outputId": "eebc63b4-2cd4-4e63-bab0-aaac54bec86e"
      },
      "source": [
        "!pip install -q -U trax\n",
        "import trax\n",
        "from trax import layers as tl"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 634kB 4.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 6.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3MB 8.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 29.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 256kB 40.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 28.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 47.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.5MB 31.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 36.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 50.2MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfTbfKWnQqmX"
      },
      "source": [
        "Trax allows to define neural network architectures by stacking layers (similarly to other libraries such as Keras). For this the `Serial()` is often used as it is a combinator that allows to stack layers serially using function composition.\n",
        "\n",
        "Next you can see a simple vanilla NN architecture containing 1 hidden(dense) layer with 128 cells and output (dense) layer with 10 cells on which we apply the final layer of logsoftmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re4ZlM8xQl2D"
      },
      "source": [
        "mlp = tl.Serial(\n",
        "  tl.Dense(128),\n",
        "  tl.Relu(),\n",
        "  tl.Dense(10),\n",
        "  tl.LogSoftmax()\n",
        ")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8QJNnMEQybn"
      },
      "source": [
        "Each of the layers within the `Serial` combinator layer is considered a sublayer. Notice that unlike similar libraries, **in Trax the activation functions are considered layers.** To know more about the `Serial` layer check the docs [here](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial).\n",
        "\n",
        "You can try printing this object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efv-JOI7Qu7-",
        "outputId": "0e5ffe66-789d-4155-fbff-4fd9bce2a3a9"
      },
      "source": [
        "print(mlp)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Dense_128\n",
            "  Serial[\n",
            "    Relu\n",
            "  ]\n",
            "  Dense_10\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS6RCXulQ62z"
      },
      "source": [
        "Printing the model gives you the exact same information as the model's definition itself.\n",
        "\n",
        "By just looking at the definition you can clearly see what is going on inside the neural network. Trax is very straightforward in the way a network is defined, that is one of the things that makes it awesome! \n",
        "\n",
        "## GRU MODEL\n",
        "\n",
        "To create a `GRU` model you will need to be familiar with the following layers (Documentation link attached with each layer name):\n",
        "   - [`ShiftRight`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight) Shifts the tensor to the right by padding on axis 1. The `mode` should be specified and it refers to the context in which the model is being used. Possible values are: 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to \"train\".\n",
        "   \n",
        "   - [`Embedding`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding) Maps discrete tokens to vectors. It will have shape `(vocabulary length X dimension of output vectors)`. The dimension of output vectors (also called `d_feature`) is the number of elements in the word embedding.\n",
        "   - [`GRU`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.GRU) The GRU layer. It leverages another Trax layer called [`GRUCell`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.GRUCell). The number of GRU units should be specified and should match the number of elements in the word embedding. If you want to stack two consecutive GRU layers, it can be done by using python's list comprehension.\n",
        "   - [`Dense`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) Vanilla Dense layer.\n",
        "   - [`LogSoftMax`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax) Log Softmax function.\n",
        "\n",
        "Putting everything together the GRU model will look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCaafQraQ0kv"
      },
      "source": [
        "mode = 'train'\n",
        "vocab_size = 256\n",
        "model_dimension = 512\n",
        "n_layers = 2\n",
        "\n",
        "GRU = tl.Serial(\n",
        "      tl.ShiftRight(mode=mode), # Do remember to pass the mode parameter if you are using it for interence/test as default is train \n",
        "      tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n",
        "      [tl.GRU(n_units=model_dimension) for _ in range(n_layers)], # You can play around n_layers if you want to stack more GRU layers together\n",
        "      tl.Dense(n_units=vocab_size),\n",
        "      tl.LogSoftmax()\n",
        "    )"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a83HYGBgSTwt",
        "outputId": "2a1a97db-2aef-4352-8955-49823829ceaf"
      },
      "source": [
        "def show_layers(model, layer_prefix=\"Serial.sublayers\"):\n",
        "    print(f\"Total layers: {len(model.sublayers)}\\n\")\n",
        "    for i in range(len(model.sublayers)):\n",
        "        print('========')\n",
        "        print(f'{layer_prefix}_{i}: {model.sublayers[i]}\\n')\n",
        "        \n",
        "show_layers(GRU)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total layers: 6\n",
            "\n",
            "========\n",
            "Serial.sublayers_0: Serial[\n",
            "  ShiftRight(1)\n",
            "]\n",
            "\n",
            "========\n",
            "Serial.sublayers_1: Embedding_256_512\n",
            "\n",
            "========\n",
            "Serial.sublayers_2: GRU_512\n",
            "\n",
            "========\n",
            "Serial.sublayers_3: GRU_512\n",
            "\n",
            "========\n",
            "Serial.sublayers_4: Dense_256\n",
            "\n",
            "========\n",
            "Serial.sublayers_5: LogSoftmax\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIpJOayURcmA"
      },
      "source": [
        "# example\n",
        "\n",
        "vocab_size = 5\n",
        "word_ids = np.array([1, 1, 3, 4], dtype=np.int32)  # word_ids < vocab_size\n",
        "embedding_layer = tl.Embedding(vocab_size, 10)\n",
        "embedding_layer.init(trax.shapes.signature(word_ids))\n",
        "embedded = embedding_layer(word_ids)  # embedded.shape = (4, 32)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8t8W7CwRr9R",
        "outputId": "4668a618-7d54-415c-9e32-2d9b2988919c"
      },
      "source": [
        "embedded"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-0.51289576, -0.47462976,  0.4763662 ,  0.5279791 ,\n",
              "               0.2914062 , -0.08452899, -0.11147123, -0.5470346 ,\n",
              "               0.02848748,  0.38207462],\n",
              "             [-0.51289576, -0.47462976,  0.4763662 ,  0.5279791 ,\n",
              "               0.2914062 , -0.08452899, -0.11147123, -0.5470346 ,\n",
              "               0.02848748,  0.38207462],\n",
              "             [-0.22234032,  0.5045553 ,  0.23639515,  0.32686418,\n",
              "               0.40890115, -0.3673048 , -0.42598405,  0.2964807 ,\n",
              "               0.14386757, -0.320109  ],\n",
              "             [ 0.01465019,  0.02285525,  0.10677988, -0.20892927,\n",
              "              -0.10496668, -0.3220369 , -0.442741  , -0.32696432,\n",
              "               0.28951478, -0.42385626]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy2p0YyYRyAd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}