{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week4_Siamese model using Trax.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMLjfHL30Kir8nhngZfrbgM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagar9926/NLP_Specialisation/blob/main/SequenceModelling/Week4_Siamese_model_using_Trax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTHPP3plkmvr",
        "outputId": "224c00b6-6e67-4226-9449-a69fb8ab9343"
      },
      "source": [
        "!pip install -q -U trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 634kB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3MB 32.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 41.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 25.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 256kB 55.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 35.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.5MB 25.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 4.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 52.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 30.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 31.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INuGVPgvksYw"
      },
      "source": [
        "import trax\n",
        "from trax import layers as tl\n",
        "import trax.fastmath.numpy as np\n",
        "import numpy\n",
        "\n",
        "# Setting random seeds\n",
        "numpy.random.seed(10)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDRNQPWRlB-l"
      },
      "source": [
        "## L2 Normalization\n",
        "\n",
        "Before building the model you will need to define a function that applies L2 normalization to a tensor. This is very important because in this week's assignment you will create a custom loss function which expects the tensors it receives to be normalized. Luckily this is pretty straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ09KPT-k1CB"
      },
      "source": [
        "def normalize(x):\n",
        "    return x / np.sqrt(np.sum(x * x, axis=-1, keepdims=True))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slruHXXGmKr-"
      },
      "source": [
        "Notice that the denominator can be replaced by `np.linalg.norm(x, axis=-1, keepdims=True)` to achieve the same results and that Trax's numpy is being used within the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPB3ZdVQmJ6L",
        "outputId": "301350c9-0a86-4ef8-9b56-6b1e82d1338f"
      },
      "source": [
        "tensor = numpy.random.random((2,5))\n",
        "print(f'The tensor is of type: {type(tensor)}\\n\\nAnd looks like this:\\n\\n {tensor}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensor is of type: <class 'numpy.ndarray'>\n",
            "\n",
            "And looks like this:\n",
            "\n",
            " [[0.77132064 0.02075195 0.63364823 0.74880388 0.49850701]\n",
            " [0.22479665 0.19806286 0.76053071 0.16911084 0.08833981]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6bnU7GEmR6O",
        "outputId": "06cc412c-72db-4eef-e472-3b2883b988ef"
      },
      "source": [
        "tensor = numpy.random.random((2,5))\n",
        "print(f'The tensor is of type: {type(tensor)}\\n\\nAnd looks like this:\\n\\n {tensor}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensor is of type: <class 'numpy.ndarray'>\n",
            "\n",
            "And looks like this:\n",
            "\n",
            " [[0.68535982 0.95339335 0.00394827 0.51219226 0.81262096]\n",
            " [0.61252607 0.72175532 0.29187607 0.91777412 0.71457578]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_FcTtqEmulx",
        "outputId": "bb802267-3599-41d5-b58d-ce6f39eb47ab"
      },
      "source": [
        "norm_tensor = normalize(tensor)\n",
        "print(f'The normalized tensor is of type: {type(norm_tensor)}\\n\\nAnd looks like this:\\n\\n {norm_tensor}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The normalized tensor is of type: <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "\n",
            "And looks like this:\n",
            "\n",
            " [[0.45177674 0.6284596  0.00260263 0.33762783 0.5356649 ]\n",
            " [0.40091467 0.47240815 0.1910407  0.6007077  0.46770892]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcDCUNaPnL3K"
      },
      "source": [
        "## Siamese Model\n",
        "\n",
        "To create a `Siamese` model you will first need to create a LSTM model using the `Serial` combinator layer and then use another combinator layer called `Parallel` to create the Siamese model. You should be familiar with the following layers (notice each layer can be clicked to go to the docs):\n",
        "   - [`Serial`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial) A combinator layer that allows to stack layers serially using function composition.\n",
        "   - [`Embedding`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding) Maps discrete tokens to vectors. It will have shape `(vocabulary length X dimension of output vectors)`. The dimension of output vectors (also called `d_feature`) is the number of elements in the word embedding.\n",
        "   - [`LSTM`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM) The LSTM layer. It leverages another Trax layer called [`LSTMCell`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTMCell). The number of units should be specified and should match the number of elements in the word embedding.\n",
        "   - [`Mean`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean) Computes the mean across a desired axis. Mean uses one tensor axis to form groups of values and replaces each group with the mean value of that group.\n",
        "   - [`Fn`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn) Layer with no weights that applies the function f, which should be specified using a lambda syntax. \n",
        "   - [`Parallel`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Parallel) It is a combinator layer (like `Serial`) that applies a list of layers in parallel to its inputs.\n",
        "\n",
        "Putting everything together the Siamese model will look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0nJ_CsFmw6l"
      },
      "source": [
        "vocab_size = 500\n",
        "model_dimension = 128\n",
        "\n",
        "# Define the LSTM model\n",
        "LSTM = tl.Serial(\n",
        "        tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n",
        "        tl.LSTM(model_dimension),\n",
        "        tl.Mean(axis=1),\n",
        "        tl.Fn('Normalize', lambda x: normalize(x))\n",
        "    )\n",
        "\n",
        "# Use the Parallel combinator to create a Siamese model out of the LSTM \n",
        "Siamese = tl.Parallel(LSTM, LSTM)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybeoz765nQCc",
        "outputId": "75a774d8-080d-4a07-b48e-47d190976e61"
      },
      "source": [
        "dir(LSTM)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_caller',\n",
              " '_clear_init_cache',\n",
              " '_do_custom_gradients',\n",
              " '_forward_abstract',\n",
              " '_init_cached',\n",
              " '_jit_cache',\n",
              " '_n_in',\n",
              " '_n_inputs_n_outputs',\n",
              " '_n_layers',\n",
              " '_n_out',\n",
              " '_name',\n",
              " '_rng',\n",
              " '_rng_seed_int',\n",
              " '_settable_attrs',\n",
              " '_state',\n",
              " '_sublayers',\n",
              " '_sublayers_to_print',\n",
              " '_validate_forward_inputs',\n",
              " '_weights',\n",
              " 'backward',\n",
              " 'forward',\n",
              " 'has_backward',\n",
              " 'init',\n",
              " 'init_from_file',\n",
              " 'init_weights_and_state',\n",
              " 'n_in',\n",
              " 'n_out',\n",
              " 'name',\n",
              " 'output_signature',\n",
              " 'pure_fn',\n",
              " 'rng',\n",
              " 'save_to_file',\n",
              " 'state',\n",
              " 'sublayers',\n",
              " 'weights',\n",
              " 'weights_and_state_signature']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZOLBGIvnVTW",
        "outputId": "128b74d2-7c7f-4978-b262-0244d33b5f7c"
      },
      "source": [
        "def show_layers(model, layer_prefix):\n",
        "    print(f\"Total layers: {len(model.sublayers)}\\n\")\n",
        "    for i in range(len(model.sublayers)):\n",
        "        print('========')\n",
        "        print(f'{layer_prefix}_{i}: {model.sublayers[i]}\\n')\n",
        "\n",
        "print('Siamese model:\\n')\n",
        "show_layers(Siamese, 'Parallel.sublayers')\n",
        "\n",
        "print('Detail of LSTM models:\\n')\n",
        "show_layers(LSTM, 'Serial.sublayers')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Siamese model:\n",
            "\n",
            "Total layers: 2\n",
            "\n",
            "========\n",
            "Parallel.sublayers_0: Serial[\n",
            "  Embedding_500_128\n",
            "  LSTM_128\n",
            "  Mean\n",
            "  Normalize\n",
            "]\n",
            "\n",
            "========\n",
            "Parallel.sublayers_1: Serial[\n",
            "  Embedding_500_128\n",
            "  LSTM_128\n",
            "  Mean\n",
            "  Normalize\n",
            "]\n",
            "\n",
            "Detail of LSTM models:\n",
            "\n",
            "Total layers: 4\n",
            "\n",
            "========\n",
            "Serial.sublayers_0: Embedding_500_128\n",
            "\n",
            "========\n",
            "Serial.sublayers_1: LSTM_128\n",
            "\n",
            "========\n",
            "Serial.sublayers_2: Mean\n",
            "\n",
            "========\n",
            "Serial.sublayers_3: Normalize\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT0VMPy8mF39"
      },
      "source": [
        "Computing the Cost II\n",
        "Now that you have the matrix with cosine similarity scores, which is the product of two matrices, we go ahead and compute the cost. \n",
        "\n",
        "\n",
        "We now introduce two concepts, the mean_neg, which is the mean negative of all the other off diagonals in the row, and the closest_neg, which corresponds to the highest number in the off diagonals. \n",
        "\n",
        "\n",
        " $ Cost=max(−cos(A,P)+cos(A,N)+α,0)$\n",
        "\n",
        "So we will have two costs now: \n",
        "\n",
        " $Cost1=max(−cos(A,P)+mean \n",
        "n\n",
        "​\t\n",
        " eg)+α,0)$\n",
        "\n",
        "$Cost2=max(−cos(A,P)+closest \n",
        "n\n",
        "​\t\n",
        " eg+α,0)$\n",
        "\n",
        "The full cost is defined as: Cost 1 + Cost 2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZR4duOcncQ3"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hQ9Xvhon-0g"
      },
      "source": [
        "#https://www.coursera.org/lecture/convolutional-neural-networks/triplet-loss-HuUtN\n",
        "## Similarity Scores\n",
        "The first step is to calculate the matrix of similarity scores using cosine similarity so that you can look up $\\mathrm{s}(A,P)$, $\\mathrm{s}(A,N)$ as needed for the loss formulas.\n",
        "\n",
        "### Two Vectors\n",
        "First I'll show you how to calculate the similarity score, using cosine similarity, for 2 vectors.\n",
        "\n",
        "$\\mathrm{s}(v_1,v_2) = \\mathrm{cosine \\ similarity}(v_1,v_2) = \\frac{v_1 \\cdot v_2}{||v_1||~||v_2||}$\n",
        "* Try changing the values in the second vector to see how it changes the cosine similarity.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs77fThqn-Ki",
        "outputId": "164686bc-0adf-4bed-9893-6ed09f64434c"
      },
      "source": [
        "# Two vector example\n",
        "# Input data\n",
        "print(\"-- Inputs --\")\n",
        "v1 = np.array([1, 2, 3], dtype=float)\n",
        "v2 = np.array([1, 2, 3.5])  # notice the 3rd element is offset by 0.5\n",
        "### START CODE HERE ###\n",
        "# Try modifying the vector v2 to see how it impacts the cosine similarity\n",
        "# v2 = v1                   # identical vector\n",
        "# v2 = v1 * -1              # opposite vector\n",
        "# v2 = np.array([0,-42,1])  # random example\n",
        "### END CODE HERE ###\n",
        "print(\"v1 :\", v1)\n",
        "print(\"v2 :\", v2, \"\\n\")\n",
        "\n",
        "# Similarity score\n",
        "def cosine_similarity(v1, v2):\n",
        "    numerator = np.dot(v1, v2)\n",
        "    denominator = np.sqrt(np.dot(v1, v1)) * np.sqrt(np.dot(v2, v2))\n",
        "    return numerator / denominator\n",
        "\n",
        "print(\"-- Outputs --\")\n",
        "print(\"cosine similarity :\", cosine_similarity(v1, v2))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Inputs --\n",
            "v1 : [1. 2. 3.]\n",
            "v2 : [1.  2.  3.5] \n",
            "\n",
            "-- Outputs --\n",
            "cosine similarity : 0.9974086507360697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u2SU1v4oNXj"
      },
      "source": [
        "### Two Batches of Vectors\n",
        "Now i'll show you how to calculate the similarity scores, using cosine similarity, for 2 batches of vectors. These are rows of individual vectors, just like in the example above, but stacked vertically into a matrix. They would look like the image below for a batch size (row count) of 4 and embedding size (column count) of 5.\n",
        "\n",
        "The data is setup so that $v_{1\\_1}$ and $v_{2\\_1}$ represent duplicate inputs, but they are not duplicates with any other rows in the batch. This means $v_{1\\_1}$ and $v_{2\\_1}$ (green and green) have more similar vectors than say $v_{1\\_1}$ and $v_{2\\_2}$ (green and magenta).\n",
        "\n",
        "I'll show you two different methods for calculating the matrix of similarities from 2 batches of vectors.\n",
        "\n",
        "<img src = 'https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/raw/d562105e68a0b85012ad3ebbb29b2af6344ad4e5/Natural%20Language%20Processing%20with%20Sequence%20Models/Week%204/v1v2_stacked.png' width=\"width\" height=\"height\" style=\"height:250px;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqLWdfbDoJVi",
        "outputId": "230a7651-0f08-4805-dd71-4fc260c533e0"
      },
      "source": [
        "# Two batches of vectors example\n",
        "# Input data\n",
        "print(\"-- Inputs --\")\n",
        "v1_1 = np.array([1, 2, 3])\n",
        "v1_2 = np.array([9, 8, 7])\n",
        "v1_3 = np.array([-1, -4, -2])\n",
        "v1_4 = np.array([1, -7, 2])\n",
        "v1 = np.vstack([v1_1, v1_2, v1_3, v1_4])\n",
        "print(\"v1 :\")\n",
        "print(v1, \"\\n\")\n",
        "v2_1 = v1_1 + np.random.normal(0, 2, 3)  # add some noise to create approximate duplicate\n",
        "v2_2 = v1_2 + np.random.normal(0, 2, 3)\n",
        "v2_3 = v1_3 + np.random.normal(0, 2, 3)\n",
        "v2_4 = v1_4 + np.random.normal(0, 2, 3)\n",
        "v2 = np.vstack([v2_1, v2_2, v2_3, v2_4])\n",
        "print(\"v2 :\")\n",
        "print(v2, \"\\n\")\n",
        "\n",
        "# Batch sizes must match\n",
        "b = len(v1)\n",
        "print(\"batch sizes match :\", b == len(v2), \"\\n\")\n",
        "\n",
        "# Similarity scores\n",
        "print(\"-- Outputs --\")\n",
        "# Option 1 : nested loops and the cosine similarity function\n",
        "sim_1 = np.zeros([b, b])  # empty array to take similarity scores\n",
        "# Loop\n",
        "for row in range(0, sim_1.shape[0]):\n",
        "    for col in range(0, sim_1.shape[1]):\n",
        "        sim_1[row, col] = cosine_similarity(v1[row], v2[col])\n",
        "\n",
        "print(\"option 1 : loop\")\n",
        "print(sim_1, \"\\n\")\n",
        "\n",
        "# Option 2 : vector normalization and dot product\n",
        "def norm(x):\n",
        "    return x / np.sqrt(np.sum(x * x, axis=1, keepdims=True))\n",
        "\n",
        "sim_2 = np.dot(norm(v1), norm(v2).T)\n",
        "\n",
        "print(\"option 2 : vec norm & dot product\")\n",
        "print(sim_2, \"\\n\")\n",
        "\n",
        "# Check\n",
        "print(\"outputs are the same :\", np.allclose(sim_1, sim_2))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Inputs --\n",
            "v1 :\n",
            "[[ 1  2  3]\n",
            " [ 9  8  7]\n",
            " [-1 -4 -2]\n",
            " [ 1 -7  2]] \n",
            "\n",
            "v2 :\n",
            "[[ 0.81856261  1.65322675  6.09761159]\n",
            " [10.74534445  6.33807987  5.49645145]\n",
            " [-2.11749402 -3.42562611 -0.29569505]\n",
            " [ 2.50984188 -5.97393493  1.09060558]] \n",
            "\n",
            "batch sizes match : True \n",
            "\n",
            "-- Outputs --\n",
            "option 1 : loop\n",
            "[[ 0.94048561  0.78244173 -0.65230963 -0.25080144]\n",
            " [ 0.71311807  0.97898313 -0.8628886  -0.19196122]\n",
            " [-0.67229468 -0.75378801  0.88687052  0.63778343]\n",
            " [ 0.03078571 -0.22588127  0.71681211  0.96319011]] \n",
            "\n",
            "option 2 : vec norm & dot product\n",
            "[[ 0.94048561  0.78244173 -0.65230963 -0.25080144]\n",
            " [ 0.71311807  0.97898313 -0.8628886  -0.19196122]\n",
            " [-0.67229468 -0.75378801  0.88687052  0.63778343]\n",
            " [ 0.03078571 -0.22588127  0.71681211  0.96319011]] \n",
            "\n",
            "outputs are the same : True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TFYg_jqzkHM"
      },
      "source": [
        "## Hard Negative Mining\n",
        "\n",
        "I'll now show you how to calculate the mean negative $mean\\_neg$ and the closest negative $close\\_neg$ used in calculating $\\mathcal{L_\\mathrm{1}}$ and $\\mathcal{L_\\mathrm{2}}$.\n",
        "\n",
        "\n",
        "$\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n",
        "\n",
        "$\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n",
        "\n",
        "You'll do this using the matrix of similarity scores you already know how to make, like the example below for a batch size of 4. The diagonal of the matrix contains all the $\\mathrm{s}(A,P)$ values, similarities from duplicate question pairs (aka Positives). This is an important attribute for the calculations to follow.\n",
        "\n",
        "<img src = 'https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/raw/d562105e68a0b85012ad3ebbb29b2af6344ad4e5/Natural%20Language%20Processing%20with%20Sequence%20Models/Week%204/ss_matrix.png' width=\"width\" height=\"height\" style=\"height:250px;\"/>\n",
        "\n",
        "\n",
        "### Mean Negative\n",
        "$mean\\_neg$ is the average of the off diagonals, the $\\mathrm{s}(A,N)$ values, for each row.\n",
        "\n",
        "### Closest Negative\n",
        "$closest\\_neg$ is the largest off diagonal value, $\\mathrm{s}(A,N)$, that is smaller than the diagonal $\\mathrm{s}(A,P)$ for each row.\n",
        "* Try using a different matrix of similarity scores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SXvvK3uzcBa",
        "outputId": "1640bcdd-6e85-436c-adae-0a8c01417fea"
      },
      "source": [
        "# Hardcoded matrix of similarity scores\n",
        "sim_hardcoded = np.array(\n",
        "    [\n",
        "        [0.9, -0.8, 0.3, -0.5],\n",
        "        [-0.4, 0.5, 0.1, -0.1],\n",
        "        [0.3, 0.1, -0.4, -0.8],\n",
        "        [-0.5, -0.2, -0.7, 0.5],\n",
        "    ]\n",
        ")\n",
        "\n",
        "sim = sim_hardcoded\n",
        "### START CODE HERE ###\n",
        "# Try using different values for the matrix of similarity scores\n",
        "# sim = 2 * np.random.random_sample((b,b)) -1   # random similarity scores between -1 and 1\n",
        "# sim = sim_2                                   # the matrix calculated previously\n",
        "### END CODE HERE ###\n",
        "\n",
        "# Batch size\n",
        "b = sim.shape[0]\n",
        "\n",
        "print(\"-- Inputs --\")\n",
        "print(\"sim :\")\n",
        "print(sim)\n",
        "print(\"shape :\", sim.shape, \"\\n\")\n",
        "\n",
        "# Positives\n",
        "# All the s(A,P) values : similarities from duplicate question pairs (aka Positives)\n",
        "# These are along the diagonal\n",
        "sim_ap = np.diag(sim)\n",
        "print(\"sim_ap :\")\n",
        "print(np.diag(sim_ap), \"\\n\")\n",
        "\n",
        "# Negatives\n",
        "# all the s(A,N) values : similarities the non duplicate question pairs (aka Negatives)\n",
        "# These are in the off diagonals\n",
        "sim_an = sim - np.diag(sim_ap)\n",
        "print(\"sim_an :\")\n",
        "print(sim_an, \"\\n\")\n",
        "\n",
        "print(\"-- Outputs --\")\n",
        "# Mean negative\n",
        "# Average of the s(A,N) values for each row\n",
        "mean_neg = np.sum(sim_an, axis=1, keepdims=True) / (b - 1)\n",
        "print(\"mean_neg :\")\n",
        "print(mean_neg, \"\\n\")\n",
        "\n",
        "# Closest negative\n",
        "# Max s(A,N) that is <= s(A,P) for each row\n",
        "mask_1 = np.identity(b) == 1            # mask to exclude the diagonal\n",
        "mask_2 = sim_an > sim_ap.reshape(b, 1)  # mask to exclude sim_an > sim_ap\n",
        "mask = mask_1 | mask_2\n",
        "sim_an_masked = np.copy(sim_an)         # create a copy to preserve sim_an\n",
        "sim_an_masked[mask] = -2\n",
        "\n",
        "closest_neg = np.max(sim_an_masked, axis=1, keepdims=True)\n",
        "print(\"closest_neg :\")\n",
        "print(closest_neg, \"\\n\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Inputs --\n",
            "sim :\n",
            "[[ 0.9 -0.8  0.3 -0.5]\n",
            " [-0.4  0.5  0.1 -0.1]\n",
            " [ 0.3  0.1 -0.4 -0.8]\n",
            " [-0.5 -0.2 -0.7  0.5]]\n",
            "shape : (4, 4) \n",
            "\n",
            "sim_ap :\n",
            "[[ 0.9  0.   0.   0. ]\n",
            " [ 0.   0.5  0.   0. ]\n",
            " [ 0.   0.  -0.4  0. ]\n",
            " [ 0.   0.   0.   0.5]] \n",
            "\n",
            "sim_an :\n",
            "[[ 0.  -0.8  0.3 -0.5]\n",
            " [-0.4  0.   0.1 -0.1]\n",
            " [ 0.3  0.1  0.  -0.8]\n",
            " [-0.5 -0.2 -0.7  0. ]] \n",
            "\n",
            "-- Outputs --\n",
            "mean_neg :\n",
            "[[-0.33333333]\n",
            " [-0.13333333]\n",
            " [-0.13333333]\n",
            " [-0.46666667]] \n",
            "\n",
            "closest_neg :\n",
            "[[ 0.3]\n",
            " [ 0.1]\n",
            " [-0.8]\n",
            " [-0.2]] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bt-ls1G6mcI"
      },
      "source": [
        "## The Loss Functions\n",
        "\n",
        "The last step is to calculate the loss functions.\n",
        "\n",
        "$\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n",
        "\n",
        "$\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n",
        "\n",
        "$\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBr6ymnT6al_",
        "outputId": "1df0fe60-cab0-479c-e019-6c1ef2b94702"
      },
      "source": [
        "# Alpha margin\n",
        "alpha = 0.25\n",
        "\n",
        "# Modified triplet loss\n",
        "# Loss 1\n",
        "l_1 = np.maximum(mean_neg - sim_ap.reshape(b, 1) + alpha, 0)\n",
        "# Loss 2\n",
        "l_2 = np.maximum(closest_neg - sim_ap.reshape(b, 1) + alpha, 0)\n",
        "# Loss full\n",
        "l_full = l_1 + l_2\n",
        "# Cost\n",
        "cost = np.sum(l_full)\n",
        "\n",
        "print(\"-- Outputs --\")\n",
        "print(\"loss full :\")\n",
        "print(l_full, \"\\n\")\n",
        "print(\"cost :\", \"{:.3f}\".format(cost))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Outputs --\n",
            "loss full :\n",
            "[[0.        ]\n",
            " [0.        ]\n",
            " [0.51666667]\n",
            " [0.        ]] \n",
            "\n",
            "cost : 0.517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsoLzLI8aeGI"
      },
      "source": [
        "# Evaluate a Siamese model: Ungraded Lecture Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r70CYQ5Za2Qi"
      },
      "source": [
        "import trax.fastmath.numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nl4N5aWa8CC"
      },
      "source": [
        "## Inspecting the necessary elements\n",
        "\n",
        "In this lecture notebook you will learn how to evaluate a Siamese model using the accuracy metric. Because there are many steps before evaluating a Siamese network (as you will see in this week's assignment) the necessary elements and variables are replicated here using real data from the assignment:\n",
        "\n",
        "   - `q1`: vector with dimension `(batch_size X max_length)` containing first questions to compare in the test set.\n",
        "   - `q2`: vector with dimension `(batch_size X max_length)` containing second questions to compare in the test set.\n",
        "   \n",
        "   **Notice that for each pair of vectors within a batch $([q1_1, q1_2, q1_3, ...]$, $[q2_1, q2_2,q2_3, ...])$  $q1_i$ is associated to $q2_k$.**\n",
        "        \n",
        "        \n",
        "   - `y_test`: 1 if  $q1_i$ and $q2_k$ are duplicates, 0 otherwise.\n",
        "   \n",
        "   - `v1`: output vector from the model's prediction associated with the first questions.\n",
        "   - `v2`: output vector from the model's prediction associated with the second questions.\n",
        "\n",
        "You can inspect each one of these variables by running the following cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYUfThfra4Bq",
        "outputId": "d1a9f00b-8333-4ade-a06e-cb87d1dc6b4f"
      },
      "source": [
        "!git clone https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Natural-Language-Processing-Specialization'...\n",
            "remote: Enumerating objects: 586, done.\u001b[K\n",
            "remote: Total 586 (delta 0), reused 0 (delta 0), pack-reused 586\u001b[K\n",
            "Receiving objects: 100% (586/586), 292.43 MiB | 34.09 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "Checking out files: 100% (496/496), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIvIGnWbbTKw",
        "outputId": "3ca186e5-7e4d-4a21-8664-94c0b3248ee6"
      },
      "source": [
        "cd /content/Natural-Language-Processing-Specialization/Natural Language Processing with Sequence Models/Week 4"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Natural-Language-Processing-Specialization/Natural Language Processing with Sequence Models/Week 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SnfjhQSbheB",
        "outputId": "0c3ecad5-f293-409b-a4e9-64c1ebd0436f"
      },
      "source": [
        "q1 = np.load('q1.npy')\n",
        "print(f'q1 has shape: {q1.shape} \\n\\nAnd it looks like this: \\n\\n {q1}\\n\\n')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "q1 has shape: (512, 64) \n",
            "\n",
            "And it looks like this: \n",
            "\n",
            " [[ 32  38   4 ...   1   1   1]\n",
            " [ 30 156  78 ...   1   1   1]\n",
            " [ 32  38   4 ...   1   1   1]\n",
            " ...\n",
            " [ 32  33   4 ...   1   1   1]\n",
            " [ 30 156 317 ...   1   1   1]\n",
            " [ 30 156   6 ...   1   1   1]]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XearLrBNbjFq",
        "outputId": "7cf24d33-c61f-414b-9dc3-d2cc5cf775da"
      },
      "source": [
        "q2 = np.load('q2.npy')\n",
        "print(f'q2 has shape: {q2.shape} \\n\\nAnd looks like this: \\n\\n {q2}\\n\\n')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "q2 has shape: (512, 64) \n",
            "\n",
            "And looks like this: \n",
            "\n",
            " [[   30   156    78 ...     1     1     1]\n",
            " [  283   156    78 ...     1     1     1]\n",
            " [   32    38     4 ...     1     1     1]\n",
            " ...\n",
            " [   32    33     4 ...     1     1     1]\n",
            " [   30   156    78 ...     1     1     1]\n",
            " [   30   156 10596 ...     1     1     1]]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCbZCgSUbnHb",
        "outputId": "d547491f-f3da-4ec2-aa28-41d6449bc661"
      },
      "source": [
        "y_test = np.load('y_test.npy')\n",
        "print(f'y_test has shape: {y_test.shape} \\n\\nAnd looks like this: \\n\\n {y_test}\\n\\n')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_test has shape: (512,) \n",
            "\n",
            "And looks like this: \n",
            "\n",
            " [0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0\n",
            " 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1\n",
            " 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB6dAcJEbqIj",
        "outputId": "b713566f-4f27-4a6e-9057-274c01c4a31f"
      },
      "source": [
        "v1 = np.load('v1.npy')\n",
        "print(f'v1 has shape: {v1.shape} \\n\\nAnd looks like this: \\n\\n {v1}\\n\\n')\n",
        "v2 = np.load('v2.npy')\n",
        "print(f'v2 has shape: {v2.shape} \\n\\nAnd looks like this: \\n\\n {v2}\\n\\n')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "v1 has shape: (512, 128) \n",
            "\n",
            "And looks like this: \n",
            "\n",
            " [[ 0.01273625 -0.1496373  -0.01982759 ...  0.02205012 -0.00169148\n",
            "  -0.01598107]\n",
            " [-0.05592084  0.05792497 -0.02226785 ...  0.08156938 -0.02570007\n",
            "  -0.00503111]\n",
            " [ 0.05686752  0.0294889   0.04522024 ...  0.03141788 -0.08459651\n",
            "  -0.00968536]\n",
            " ...\n",
            " [ 0.15115018  0.17791134  0.02200656 ... -0.00851707  0.00571415\n",
            "  -0.00431194]\n",
            " [ 0.06995274  0.13110274  0.0202337  ... -0.00902792 -0.01221745\n",
            "   0.00505962]\n",
            " [-0.16043712 -0.11899089 -0.15950686 ...  0.06544471 -0.01208312\n",
            "  -0.01183368]]\n",
            "\n",
            "\n",
            "v2 has shape: (512, 128) \n",
            "\n",
            "And looks like this: \n",
            "\n",
            " [[ 0.07437647  0.02804951 -0.02974014 ...  0.02378932 -0.01696189\n",
            "  -0.01897198]\n",
            " [ 0.03270066  0.15122835 -0.02175895 ...  0.00517202 -0.14617395\n",
            "   0.00204823]\n",
            " [ 0.05635608  0.05454165  0.042222   ...  0.03831453 -0.05387777\n",
            "  -0.01447786]\n",
            " ...\n",
            " [ 0.04727105 -0.06748016  0.04194937 ...  0.07600753 -0.03072828\n",
            "   0.00400715]\n",
            " [ 0.00269269  0.15222628  0.01714724 ...  0.01482705 -0.0197884\n",
            "   0.01389528]\n",
            " [-0.15475044 -0.15718803 -0.14732707 ...  0.04299919 -0.01070975\n",
            "  -0.01318042]]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8lhXVNPb04R"
      },
      "source": [
        "## Calculating the accuracy\n",
        "\n",
        "You will calculate the accuracy by iterating over the test set and checking if the model predicts right or wrong.\n",
        "\n",
        "The first step is to set the accuracy to zero:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xql_ROSobwmr"
      },
      "source": [
        "accuracy = 0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Nz1XfOb76N"
      },
      "source": [
        "You will also need the `batch size` and the `threshold` that determines if two questions are the same or not. \n",
        "\n",
        "**Note :A higher threshold means that only very similar questions will be considered as the same question.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cfiuYEnb43R"
      },
      "source": [
        "batch_size = 512 # Note: The max it can be is y_test.shape[0] i.e all the samples in test data\n",
        "threshold = 0.7 # You can play around with threshold and then see the change in accuracy.\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFm-dh4dcNMF"
      },
      "source": [
        "In the assignment you will iterate over multiple batches of data but since this is a simplified version only one batch is provided. \n",
        "\n",
        "**Note: Be careful with the indices when slicing the test data in the assignment!**\n",
        "\n",
        "The process is pretty straightforward:\n",
        "   - Iterate over each one of the elements in the batch\n",
        "   - Compute the cosine similarity between the predictions\n",
        "       - For computing the cosine similarity, the two output vectors should have been normalized using L2 normalization meaning their magnitude will be 1. This has been taken care off by the Siamese network you will build in the assignment. Hence the cosine similarity here is just dot product between two vectors. You can check by implementing the usual cosine similarity formula and check if this holds or not.\n",
        "   - Determine if this value is greater than the threshold (If it is, consider the two questions as the same and return 1 else 0)\n",
        "   - Compare against the actual target and if the prediction matches, add 1 to the accuracy (increment the correct prediction counter)\n",
        "   - Divide the accuracy by the number of processed elements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wbsk84qQb-dz",
        "outputId": "ac0f9d9f-7105-4820-d2bf-06b5b2b24242"
      },
      "source": [
        "for j in range(batch_size):        # Iterate over each one of the elements in the batch\n",
        "    \n",
        "    d = np.dot(v1[j],v2[j])        # Compute the cosine similarity between the predictions as l2 normalized, ||v1[j]||==||v2[j]||==1 so only dot product is needed\n",
        "    res = d > threshold            # Determine if this value is greater than the threshold (if it is consider the two questions as the same)\n",
        "    accuracy += (y_test[j] == res) # Compare against the actual target and if the prediction matches, add 1 to the accuracy\n",
        "\n",
        "accuracy = accuracy / batch_size   # Divide the accuracy by the number of processed elements"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9akx_yP7cPWr",
        "outputId": "e5ab59d2-9042-4b0f-a0c8-47360435f544"
      },
      "source": [
        "print(f'The accuracy of the model is: {accuracy}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of the model is: 0.7421875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ri78O9icQ8f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}